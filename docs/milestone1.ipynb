{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> AC207 Systems Development for Computational Science\n",
    "\n",
    "\n",
    "## Milestone 1 : Document\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2021**<br/>\n",
    "**Project Group #32**: Xuliang Guo.   Van Anh Le.  Hanwen Cui.  Kamran Ahmed.\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "00000-e00a22e4-f5be-4aac-8def-bed7c168d3d0",
    "deepnote_cell_type": "code",
    "execution_millis": 91,
    "execution_start": 1604200362789,
    "output_cleared": false,
    "source_hash": "9e5190f0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 {\n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left;\n",
       "    padding-left: 10px;\n",
       "    background-color: #63ACBE;\n",
       "    color: black;\n",
       "}\n",
       "h2 {\n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left;\n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE;\n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #f8b4ab;\n",
       "\tborder-color: #E9967A;\n",
       "\tborder-left: 5px solid #601A4A;\n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "div.exercise-r {\n",
       "\tbackground-color: #ffd0d0;\n",
       "\tborder-color: #E9967A;\n",
       "\tborder-left: 5px solid #601A4A;\n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #63ACBE;\n",
       "\tborder-color: #E9967A;\n",
       "\tborder-left: 5px solid #601A4A;\n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc {\n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A;\n",
       "\tborder-left: 5px solid #601A4A;\n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 {\n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left;\n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE;\n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left;\n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD;\n",
       "    color: black;\n",
       "}\n",
       "span.emph {\n",
       "\tcolor: #601A4A;\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\n",
    "    \"https://raw.githubusercontent.com/Harvard-IACS/2021-CS109A/master/\"\n",
    "    \"themes/static/css/cs109.css\"\n",
    ").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-c10da6ba-b1c7-476d-a044-0799bdfa7a18",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "## Part1. Introduction\n",
    "\n",
    "Almost all machine learning algorithms can be attributed to solving optimization problems during the training process. If the analytical solution exists, we can achieve the final result by simply substituting numerical calculation. However, not all problems are amenable to analytical solutions. Many times we need to use gradient descent, Newton method, quasi-Newton method, and other numerical optimization algorithms to solve such problems. These algorithms largely rely on the first or second derivative, gradient, Jacobian matrix, Hessian matrix, etc.\n",
    "\n",
    "- Let's define $f(x)=x^{2} + y^{2} $, and we want its derivative respect to x and y.  ie $\\frac{\\partial f}{\\partial x} $ and $\\frac{\\partial f}{\\partial y} $\n",
    "\n",
    "For programming to solve the derivative of a function, there are currently 4 methods we can choose:  numerical differentiation, manual differentiation, symbolic differentiation, and what we will introduce -automatic differentiation.\n",
    "\n",
    "> Automatic Differentiation (AD), also known as computational differentiation, is a method that combines the advantages of symbolic differentiation and numerical differentiation that can efficiently and accurately evaluate derivatives of functions that have been expressed as a computer program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2. Background\n",
    "\n",
    "\n",
    "### Numerical Differentiation\n",
    "\n",
    "\n",
    "According to the original definition of derivative \n",
    "\n",
    "$$ \\begin{aligned}\n",
    "&\\frac{\\partial f}{\\partial x}(a, b)=\\lim _{\\epsilon \\rightarrow 0} \\frac{f(a+\\epsilon, b)-f(a, b)}{\\epsilon} \\\\\n",
    "&\\frac{\\partial f}{\\partial y}(a, b)=\\lim _{\\epsilon \\rightarrow 0} \\frac{f(a, b+\\epsilon)-f(a, b)}{\\epsilon}\n",
    "\\end{aligned} $$\n",
    "\n",
    "\n",
    "\n",
    "we will get an approximate derivative value as long as $\\epsilon$ takes a very small value, such as 0.0001. In other words, the numerical method only takes three arguments (objective function, the current value, and the precision(0.0001)) to be able to output an approximate derivative for the current value. However,  the numerical differentiation method is the most computationally expensive among the four methods when there are multiple variables.\n",
    "\n",
    "From the demo python code below, we can see that the Simple_numerical_diff_x function calls f-function twice.\n",
    "```python\n",
    "def Simple_numerical_diff_x(f,x,y x_eps):\n",
    "    return (f(x + x_eps,y) - f(x,y)) / (x_eps)\n",
    "```\n",
    "For the one variable scenario above, the Simple_numerical_diff calls f() twice. For an equation with n variables, to compute the gradient of all dimensions, f() will be called at least n+1 times. When dealing with a large neural network, such operations will greatly reduce our efficiency.\n",
    "\n",
    "To make matters worse, the roundoff error and truncation error caused by the numerical approximation makes it even less suitable for practical application scenarios. Even if we make $\\epsilon$ equal to 1e-10, the roundoff error will still exist, but the calculations will slow down even more due to the sharp increase of digits. For the truncation error, it can be reduced not be eliminated by the the following center difference approximation: $f^{\\prime}(x)=\\lim _{\\epsilon \\rightarrow 0} \\frac{f(x+\\epsilon ,y)-f(x-\\epsilon ,y)}{2 \\epsilon }$. Although the numerical differentiation can not give a precise result, it is often used to check the results of other methods within a given accuracy.\n",
    "\n",
    "\n",
    "### Manual Differentiation\n",
    "\n",
    "Another method is we grab a pen and a piece of paper, manually write down the derivate formula based on our calculus knowledge and then write it into python code.\n",
    "\n",
    "$$ \\frac{\\partial f(x,y)}{\\partial x}=\\frac{\\partial\\left(x^{2}+y^{2}\\right )}{\\partial x} =2x$$\n",
    "\n",
    "However, within the manual method, every time we modify the objective function, the stored derivative expression need also be re-write. The method may become very wordy and time-costly when it is applied to more complex functions. So the manual differentiation obviously cannot be applied to automated machine learning algorithms due to the requirements of human intervention.\n",
    "\n",
    "\n",
    "### Symbolic Differentiation\n",
    "\n",
    "Symbolic differentiation is a method used to replace the above manual differentiation, using algebraic programming to implement the automatic derivative of some combined functions, such as:\n",
    "\n",
    "$$\\begin{gathered}\n",
    "\\frac{d}{d x}(f(x)+g(x))=\\frac{d}{d x} f(x)+\\frac{d}{d x} g(x) \\\\\n",
    "\\frac{d}{d x} f(x) g(x)=\\left(\\frac{d}{d x} f(x)\\right) g(x)+f(x)\\left(\\frac{d}{d x} g(x)\\right) \\\\\n",
    "\\end{gathered}$$\n",
    "\n",
    "Symbolic differentiation first expands the objective function to a tree diagram and then calculates and stores the partial derivative of all leaf nodes and stores them in memory. Treat all the stored expression as a new element, the symbolic differentiation iteratively compute the composite derivative of the parents node. After several upward iterations, we will finally obtain the symbolic expression of composited elements and it is easy to obtain the final answer by simply substituting the initial value.\n",
    "\n",
    "A significant problem with this method is \"expression swell\". For a complex objective function, the symbolic method will produce a huge calculation graph, which can't be simplified and will lead to an extremely slow calculation through the entire expression trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Numerical differentiation emphasizes the direct substitution of finite numerical approximation at the beginning; \n",
    "> - Symbolic differentiation emphasizes generating an algebraic derivative expression and then substituting the value at the final step;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation\n",
    "\n",
    "> - Automatic differentiation will apply the symbolic differentiation method to the most basic operators, such as constants, power functions, exponential functions, logarithmic functions, trigonometric functions, etc., Then substitutes the initial value and applies rules of composition derivative, we achieve the final answer of a wide class of functions automatically and precisely. Due to the basic of only applying symbolic differentiation law to basic functions, it can flexibly combine the loop structure and conditional structure of the programming language without worrying about the problem of expression swell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AD Forward Mode and Dual numbers\n",
    "AD computes the derivative of a complex function $f(x,y)$ by breaking it down into a series of intermediate elementary operations. The chain rule allows us to use the derivative of these intermediate functions to compute $\\frac{\\partial f}{\\partial x}$. In the forward mode AD, we work from the inside out, i.e starting with the value of $x$ at which we want to evaluate and ending with the function $f(x,y)$. \n",
    "\n",
    "The evaluation trace table and computational graph below demonstrate this process:\n",
    "* We'll use the function defined in the introduction, $f(x)=x^{2} + y^{2}$, and evaluate $\\frac{\\partial f}{\\partial x} $ and $\\frac{\\partial f}{\\partial y} $ at $(x,y) = (1,1)$.\n",
    "* The first two traces $x_{0}$ and $x_{1}$ are the starting values $(x,y) = (1,1)$. \n",
    "* For each subsequent trace, elementary operations are applied to the previous traces, until we reach the final function in the last trace. The elementary operations applied are represented on the arrows in the computational graph.\n",
    "* At each trace, both the derivative ($\\nabla_{x}$, $\\nabla_{y}$) and value of the intermediate operations are computed. We arrived at the derivatives $(\\nabla_{x}$, $\\nabla_{y}) = (2,2)$ in the last trace.\n",
    "\n",
    "#####  Evaluation Trace Table\n",
    "\\begin{align}\n",
    " f(x,y)=x^{2} + y^{2} \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "| trace   | elem op.    | value | elem der.           | $\\nabla_{x}$| $\\nabla_{y}$ |\n",
    "| :---:   | :------:    | :---: | :-------:           | :---------: | :----------: |\n",
    "| $x_{0}$ | $$x$$       | $$1$$ | $$\\dot{x}$$ | $$1$$         | $$0$$          |\n",
    "| $x_{1}$ | $y$         | $1$   | $\\dot{y}$   | $0$           | $1$          |\n",
    "| $v_{1}$ | $x_{1}^{2}$ | $1$   | $2x_{1}\\dot{x_{1}}$ | $0 $         | $2$          |\n",
    "| $v_{2}$ | $x_{0}^{2}$ | $1$   | $2x_{0}\\dot{x_{0}}$ | $2$          | $0$          |\n",
    "| $f$ | $v_{2} + v_{3}$ | $2$   | $\\dot{v_1}+\\dot{v_2}$ | $2$         | $2$          |\n",
    "\n",
    "\n",
    "##### Computational Graph - Forward Mode\n",
    "\n",
    "![alt text](graph1_forward.png \"forward_mode\")\n",
    "\n",
    "As we saw from the trace table, both the derivative and value of the intermediate operations are computed at each trace. Dual numbers can be used as a data structure to store and carried these values forward, where each value and derivative pair can be stored as the real part and the dual part of a dual number respectively. The addition and multiplication properties of dual numbers, as well as the fact that its derivative can be computed using the Taylor series, make it a useful data structure for forward mode AD.\n",
    "\n",
    "\n",
    "### AD Reverse Mode\n",
    "In reverse mode AD, instead of working from the inside out, we apply the chain rule backwards. To do this, we first need to perform a forward pass where the partial derivative of intermediate variables with respect to its parent are computed. We then performed a reverse pass and apply the chain rule starting from the last intermediate variable using the computed partial derivative. \n",
    "\n",
    "##### Computational Graph - Reverse Mode\n",
    "\n",
    "![alt text](graph2_reverse.png \"reverse_mode\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 3. How to Use --- autodiff\n",
    "Package will be installed from GitHub or PyPI. Users will interact with our package by instantiating `Dual` numbers and constructing user-defined functions which automatically evaluates the function at the specified values and computes the gradient(s).\n",
    "\n",
    "**Function with single argument**\n",
    "\n",
    "```python\n",
    ">>> import autodiff as ad\n",
    ">>> x = ad.Dual(2) # instatiate Dual object with value 2 (derivative defaults to 1)\n",
    ">>> f = 7 * (x ** 3) + 3 * x # define and evaluate function\n",
    ">>> f.val # function output\n",
    "62\n",
    ">>> f.der # derivative\n",
    "array([87.])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multivariable functions** \n",
    "\n",
    "We provide flexible options to support this.\n",
    "\n",
    "*Option 1*: create multiple Dual numbers via `Dual.from_array`, which constructs proper seed vectors for each variable. i.e. Dual numbers will have value of `X[i]` and zero derivative vector where the `i-th` element has a value of 1.\n",
    "\n",
    "```python\n",
    ">>> import autodiff as ad\n",
    ">>> X = [2, 4]\n",
    ">>> x, y = ad.Dual.from_array(X) # helper static method\n",
    ">>> x\n",
    "Dual(2, array([1., 0.]))\n",
    ">>> y\n",
    "Dual(4, array([0., 1.]))\n",
    ">>> f = 7 * (x ** 3) + 3 * y\n",
    ">>> f.val\n",
    "68\n",
    ">>> f.der\n",
    "array([84., 3.]) # df/dx, df/dy\n",
    "```\n",
    "\n",
    "*Option 2*: user defines seed.\n",
    "```python\n",
    ">>> import autodiff as ad\n",
    ">>> x = ad.Dual(2, der=[1, 0])\n",
    ">>> y = ad.Dual(4, der=[0, 1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Elementary operations**\n",
    "\n",
    "We will allow users to import overloaded elementary functions (sine, cosine, tangent, exponential, log, sqrt) to perform operations on Dual numbers.\n",
    "\n",
    "```python\n",
    ">>> import autodiff as ad\n",
    ">>> x = ad.Dual(0)\n",
    ">>> ad.cos(x) # overloaded function which operates on `Dual` numbers\n",
    "Dual(1, array([-0.]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Potential enhancement** \n",
    "\n",
    "Per Fabian's suggestions, we may implement a lazy evaluation method by constructing a computational graph when a user defines a function using node/graph variables and evaluate the function later. i.e.\n",
    "\n",
    "```python\n",
    ">>> import autodiff as ad\n",
    ">>> x, y = ad.graph_variables(2)\n",
    ">>> f = 7 * (x ** 3) + 3 * y # define function\n",
    ">>> f([2, 4]) # evaluate function with values for x and y\n",
    "68\n",
    ">>> f.grad([2, 4]) # compute all gradient components\n",
    "array([84., 3.])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. Software Organization\n",
    "\n",
    "**Project structure**\n",
    "\n",
    "```zsh\n",
    "cs107-FinalProject\n",
    "├── autodiff # main package where we import necessary objects and functions from `src`\n",
    "│   ├── src\n",
    "│   │   ├── extensions # extensions package (TBD)\n",
    "│   │       └── __init__.py\n",
    "│   │   └── forward # forward mode package\n",
    "│   │       ├── __init__.py\n",
    "│   │       ├── dual.py\n",
    "│   │       └── operations.py\n",
    "│   ├── tests # testing functions for all functions and methods in src\n",
    "│   │   ├── __init__.py\n",
    "│   │   ├── test_dual.py\n",
    "│   │   └── test_operations.py\n",
    "│   └── __init__.py\n",
    "├── docs\n",
    "│   ├── README.md # README for usage and tutorials\n",
    "│   └── milestone1.ipynb\n",
    "├── .gitignore\n",
    "├── .travis.yml # TravisCI build configuration\n",
    "├── LICENSE\n",
    "├── pyproject.toml # minimum build system requirements \n",
    "├── README.md # main README for installation and overview\n",
    "├── requirements.txt # necessary dependencies\n",
    "└── setup.cfg # setuptools configuration\n",
    "\n",
    "```\n",
    "\n",
    "**Modules and packages**\n",
    "- *autodiff/*: Main package where we import necessary objects and functions from *extensions* and *forward* packages. This will provide basic functionality to initialize our package (`import autodiff as ad`).\n",
    "\n",
    "\n",
    "- *autodiff/src/extensions*: Extensions package for our AD algorithm. We are still in the process of deciding what direction(s) we want to take here. Examples include reverse mode implementation, computational graph optimizations.\n",
    "\n",
    "\n",
    "- *autodiff/src/forward*: Package that implements basic forward-mode AD. Currently, this just includes a data structure named `Dual` that provides a straightforward, non-optimized AD implementation as described above. Further, we may expand upon this to include a lazy evaluation method using computational graphs.\n",
    "\n",
    "\n",
    "- *autodiff/src/tests*: This package will house our testing suite for all functions and methods in our *autodiff/src/* folder. This will be used by `pytest` and TravisCI to test our project before deployment and CodeCov to report test coverage.\n",
    "\n",
    "\n",
    "**Test suite location, TravisCI, and CodeCov**\n",
    "\n",
    "Our test suites will be located in *autodiff/src/tests* and will use the `pytest` framework to easily create simple and custom tests for all of our source code. TravisCI will be used to automatically build, test, and deploy our AD package. Build status will be include on our main README.md file and we will be notified if the build fails. `pytest` and `pytest-cov` will generate code coverage reports and metrics during the TravisCI build which will be used by CodeCov to gather and report the percentage of our code base that is covered by our testing suite. A CodeCov badge with this percentage will also be displayed on our main README.\n",
    "\n",
    "\n",
    "**Packaging and distribution**\n",
    "\n",
    "Our AD package will be built using a `PEP 517` package builder, `build`, developed by PyPA (e.g. `python3 -m build .`). This will create our source distribution and wheels. This is a simple and straightforward package builder without too much overhead for our purposes. Our AD package will be uploaded to TestPyPI and PyPI using `twine`. We also include source distribution and wheels under Releases in our GitHub repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5. Implementation\n",
    "\n",
    "Discuss how you plan on implementing the forward mode of automatic differentiation.\n",
    "> Core Data Structures**\n",
    " - Dual objects\n",
    " - numpy array \n",
    " - float\n",
    "    \n",
    "\n",
    "> Implemented Classes**\n",
    "\n",
    " - We'll need to implement Dual class for forward mode. \n",
    " \n",
    "> Method and Attributes of the Classes**\n",
    "\n",
    "```\n",
    "Dual Class\n",
    "  |\n",
    "  |\n",
    "  Attributes ───\n",
    "  |           | ─── val: float\n",
    "  |           └ ─── der: length m vector where m is the dimension of independent variable\n",
    "  |            \n",
    "  Methods    ───\n",
    "              | ─── Memeber Method: __init__(self,val,der): create a Dual object\n",
    "              |                     __add__(self,other):performs addition on current dual number\n",
    "              |                     __mul__(self,other):performs multiplication on current dual number\n",
    "              |                     __rmul__(self, other): performs multiplication from the other direction\n",
    "              |                     __sub__(self,other):performs subtraction on current dual number\n",
    "              |                     __truediv__(self,other):performs division on current dual number\n",
    "              |                     __rtruediv__(self,other):division from the other direction\n",
    "              |                     __str__(self): return string representation of current dual number\n",
    "              |                     _compatible(self, other, operand=None):Check if other element is of correct type\n",
    "              |                     ...\n",
    "              |                     \n",
    "              |\n",
    "              └ ─── Static Method:  fromArray(x):returns a list of dual numbers created from user input x\n",
    "                                    constant(val, ndim=1): Create a Dual number representing a constant\n",
    "              \n",
    "\n",
    "```  \n",
    "\n",
    "> External Dependencies**\n",
    "\n",
    " - We plan on using numpy. \n",
    "\n",
    "> deal with elementary functions like sin, sqrt, log, and exp (and all the others)**\n",
    "\n",
    " - We plan on create a module called Operations.py under the autodiff/src/forward directory. We'll define elementary functions such as sin,sqrt,log and exp in this file. When a user needs to do any operation, user will import the module and use the function defined in Operations.py. Each of the elementary function accepts integer,floats or dual number in its parameter and returns a new dual number.\n",
    "\n",
    "```\n",
    "Operation Module\n",
    "  |\n",
    "  |\n",
    "  Methods  ───\n",
    "            | ─── sin(x): returns a new Dual number after sin operation\n",
    "            | ─── cos(x): returns a new Dual number after cos operation\n",
    "            | ─── log(x,base=e): returns a new Dual number after log operation with specified base\n",
    "            | ─── exp(x): returns a new Dual number after exp operations\n",
    "           ... \n",
    "            \n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part6. Licensing\n",
    "\n",
    "> - Helper to choose a license\n",
    "> - Licenses\n",
    "> - License recommendations\n",
    "> - License compatibility\n",
    "> - Extensive list of open source licenses\n",
    "\n",
    "- We decide to use BSD 3-Clause “New” or “Revised” License. The only external dependency that we use is Numpy. Numpy is copyright under BSD 3-Clause \"New\" or \"Revised\" License. Under this license, redistribution and use is permitted if we retain the same copyright notice. So, we don't need to deal with patents. In addition, we want others to be able to use and redistribute our code as well. By using the same license, we pass along the same amount of freedom to use and distribute. \n",
    "\n",
    "```txt\n",
    "BSD 3-Clause License\n",
    "\n",
    "Copyright (c) [year], [fullname]\n",
    "All rights reserved.\n",
    "\n",
    "Redistribution and use in source and binary forms, with or without\n",
    "modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "1. Redistributions of source code must retain the above copyright notice, this\n",
    "   list of conditions and the following disclaimer.\n",
    "\n",
    "2. Redistributions in binary form must reproduce the above copyright notice,\n",
    "   this list of conditions and the following disclaimer in the documentation\n",
    "   and/or other materials provided with the distribution.\n",
    "\n",
    "3. Neither the name of the copyright holder nor the names of its\n",
    "   contributors may be used to endorse or promote products derived from\n",
    "   this software without specific prior written permission.\n",
    "\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
    "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
    "FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
    "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
    "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
    "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
    "OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 7. Milestone 1 Comments and Feedback\n",
    "\n",
    " > Good work! Really enjoyed your \"How to Use\" section, very detailed! Having a computational graph and an evaluation trace in the \"Background\" will be helpful in providing users of this package some more general information and a visualization of how AD works.\n",
    "\n",
    "Modification:\n",
    "\n",
    "- Modified and polished the background part.\n",
    "  \n",
    "- Added computational graph and evaluation trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "6e15877c-964e-4b84-bf2c-3775b98c0a30",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
